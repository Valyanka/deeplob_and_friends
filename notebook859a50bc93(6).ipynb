{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7036795,"sourceType":"datasetVersion","datasetId":4048333},{"sourceId":7067452,"sourceType":"datasetVersion","datasetId":4069664}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DeepLOB: Deep Convolutional Neural Networks for Limit Order Books\n\n### Authors: Zihao Zhang, Stefan Zohren and Stephen Roberts\nOxford-Man Institute of Quantitative Finance, Department of Engineering Science, University of Oxford\n\nThis jupyter notebook is used to demonstrate our recent paper [2] published in IEEE Transactions on Singal Processing. We use FI-2010 [1] dataset and present how model architecture is constructed here. \n\n### Data:\nThe FI-2010 is publicly avilable and interested readers can check out their paper [1]. The dataset can be downloaded from: https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649 \n\nOtherwise, the notebook will download the data automatically or it can be obtained from: \n\nhttps://drive.google.com/drive/folders/1Xen3aRid9ZZhFqJRgEMyETNazk02cNmv?usp=sharing\n\n### References:\n[1] Ntakaris A, Magris M, Kanniainen J, Gabbouj M, Iosifidis A. Benchmark dataset for mid‐price forecasting of limit order book data with machine learning methods. Journal of Forecasting. 2018 Dec;37(8):852-66. https://arxiv.org/abs/1705.03233\n\n[2] Zhang Z, Zohren S, Roberts S. DeepLOB: Deep convolutional neural networks for limit order books. IEEE Transactions on Signal Processing. 2019 Mar 25;67(11):3001-12. https://arxiv.org/abs/1808.03668\n\n### This notebook runs on Pytorch 1.9.0.","metadata":{}},{"cell_type":"code","source":"import os \nif not os.path.isfile('data.zip'):\n    !wget https://raw.githubusercontent.com/zcakhaa/DeepLOB-Deep-Convolutional-Neural-Networks-for-Limit-Order-Books/master/data/data.zip\n    !unzip -n data.zip\n    print('data downloaded.')\nelse:\n    print('data already existed.')","metadata":{"execution":{"iopub.status.busy":"2023-11-27T17:21:24.122266Z","iopub.execute_input":"2023-11-27T17:21:24.122596Z","iopub.status.idle":"2023-11-27T17:21:32.013832Z","shell.execute_reply.started":"2023-11-27T17:21:24.122559Z","shell.execute_reply":"2023-11-27T17:21:32.012485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load packages\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom tqdm import tqdm \nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom torchinfo import summary\nimport torch.nn as nn\nimport torch.optim as optim\nimport scipy\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)\n# N, D = X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:39:35.421041Z","iopub.execute_input":"2023-11-28T12:39:35.422253Z","iopub.status.idle":"2023-11-28T12:39:41.276178Z","shell.execute_reply.started":"2023-11-28T12:39:35.422211Z","shell.execute_reply":"2023-11-28T12:39:41.274891Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:39:41.278331Z","iopub.execute_input":"2023-11-28T12:39:41.279182Z","iopub.status.idle":"2023-11-28T12:39:41.285949Z","shell.execute_reply.started":"2023-11-28T12:39:41.279146Z","shell.execute_reply":"2023-11-28T12:39:41.284927Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ask_prices = []\nwith open('/kaggle/input/denis-data/rosn_data_sample/_ask_prices.txt') as f:\n    lines = f.readlines()\n    for line in lines:\n        ask_prices.append(list(map(float, line.split(','))))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:39:53.037027Z","iopub.execute_input":"2023-11-28T12:39:53.037412Z","iopub.status.idle":"2023-11-28T12:39:59.665120Z","shell.execute_reply.started":"2023-11-28T12:39:53.037382Z","shell.execute_reply":"2023-11-28T12:39:59.664007Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"ask_vols = []\nwith open('/kaggle/input/denis-data/rosn_data_sample/_ask_vols.txt') as f:\n    lines = f.readlines()\n    for line in lines:\n        ask_vols.append(list(map(float, line.split(','))))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:39:59.667187Z","iopub.execute_input":"2023-11-28T12:39:59.667569Z","iopub.status.idle":"2023-11-28T12:40:05.002192Z","shell.execute_reply.started":"2023-11-28T12:39:59.667534Z","shell.execute_reply":"2023-11-28T12:40:05.001277Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"bid_prices = []\nwith open('/kaggle/input/denis-data/rosn_data_sample/_bid_prices.txt') as f:\n    lines = f.readlines()\n    for line in lines:\n        bid_prices.append(list(map(float, line.split(','))))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:40:05.003229Z","iopub.execute_input":"2023-11-28T12:40:05.003525Z","iopub.status.idle":"2023-11-28T12:40:11.925477Z","shell.execute_reply.started":"2023-11-28T12:40:05.003499Z","shell.execute_reply":"2023-11-28T12:40:11.924576Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"bid_vols = []\nwith open('/kaggle/input/denis-data/rosn_data_sample/_bid_vols.txt') as f:\n    lines = f.readlines()\n    for line in lines:\n        bid_vols.append(list(map(float, line.split(','))))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:40:11.927964Z","iopub.execute_input":"2023-11-28T12:40:11.928368Z","iopub.status.idle":"2023-11-28T12:40:17.276493Z","shell.execute_reply.started":"2023-11-28T12:40:11.928330Z","shell.execute_reply":"2023-11-28T12:40:17.275551Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"targets = []\nwith open('/kaggle/input/denis-data/rosn_data_sample/_targets.txt') as f:\n    lines = f.readlines()\n    for line in lines:\n        targets.append(float(line))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:40:17.277562Z","iopub.execute_input":"2023-11-28T12:40:17.277844Z","iopub.status.idle":"2023-11-28T12:40:17.562714Z","shell.execute_reply.started":"2023-11-28T12:40:17.277817Z","shell.execute_reply":"2023-11-28T12:40:17.561906Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def prerprocces_data(ask_prices, ask_vols, bid_prices, bid_vols):\n    data = []\n    for time in range(len(ask_prices)):\n        cur = []\n        for level in range(10):\n            cur.extend([ask_prices[time][level], ask_vols[time][level], bid_prices[time][level], bid_vols[time][level]])\n        data.append(cur)\n    return data\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:43:30.123880Z","iopub.execute_input":"2023-11-28T12:43:30.124925Z","iopub.status.idle":"2023-11-28T12:43:30.131170Z","shell.execute_reply.started":"2023-11-28T12:43:30.124884Z","shell.execute_reply":"2023-11-28T12:43:30.130080Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"my_data = prerprocces_data(ask_prices, ask_vols, bid_prices, bid_vols)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:43:32.286993Z","iopub.execute_input":"2023-11-28T12:43:32.287402Z","iopub.status.idle":"2023-11-28T12:43:35.485386Z","shell.execute_reply.started":"2023-11-28T12:43:32.287374Z","shell.execute_reply":"2023-11-28T12:43:35.484234Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset_train = my_data[:int(np.floor(len(my_data) * 0.6))]\ntargets_train = targets[:int(np.floor(len(my_data) * 0.6))]\ndataset_val = my_data[int(np.floor(len(my_data) * 0.6)):int(np.floor(len(my_data) * 0.7))]\ntargets_val = targets[int(np.floor(len(my_data) * 0.6)):int(np.floor(len(my_data) * 0.7))]\ndataset_test = my_data[int(np.floor(len(my_data) * 0.7)):]\ntargets_test = targets[int(np.floor(len(my_data) * 0.7)):]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:45:37.063699Z","iopub.execute_input":"2023-11-28T12:45:37.064106Z","iopub.status.idle":"2023-11-28T12:45:37.078221Z","shell.execute_reply.started":"2023-11-28T12:45:37.064072Z","shell.execute_reply":"2023-11-28T12:45:37.077126Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"dataset_train = scipy.stats.zscore(dataset_train, axis=0)\nprint('train done')\ndataset_val = scipy.stats.zscore(dataset_val, axis=0)\nprint('val done')\ndataset_test = scipy.stats.zscore(dataset_test, axis=0)\nprint('test done')","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:47:12.741016Z","iopub.execute_input":"2023-11-28T12:47:12.741766Z","iopub.status.idle":"2023-11-28T12:47:15.500755Z","shell.execute_reply.started":"2023-11-28T12:47:12.741731Z","shell.execute_reply":"2023-11-28T12:47:15.499714Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"train done\nval done\ntest done\n","output_type":"stream"}]},{"cell_type":"code","source":"d = {-1.0: 0, 0.0:1, 1.0:2}\ndef make_dataset(data, targets, T):\n    time_dataset = []\n    time_targets = []\n    for i in range(T, len(data)):\n        cur = data[i-T:i]\n        time_dataset.append(cur)\n        time_targets.append(d[targets[i]])\n    return time_dataset, time_targets\n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:47:32.998911Z","iopub.execute_input":"2023-11-28T12:47:32.999329Z","iopub.status.idle":"2023-11-28T12:47:33.006257Z","shell.execute_reply.started":"2023-11-28T12:47:32.999297Z","shell.execute_reply":"2023-11-28T12:47:33.005125Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"time_dataset_train, time_targets_train = make_dataset(dataset_train, targets_train, 100)\ntime_dataset_val, time_targets_val = make_dataset(dataset_val, targets_val, 100)\ntime_dataset_test, time_targets_test = make_dataset(dataset_test, targets_test, 100)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:49:11.442178Z","iopub.execute_input":"2023-11-28T12:49:11.442640Z","iopub.status.idle":"2023-11-28T12:49:11.737492Z","shell.execute_reply.started":"2023-11-28T12:49:11.442601Z","shell.execute_reply":"2023-11-28T12:49:11.736526Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Dataset(data.Dataset):\n    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n    def __init__(self, time_dataset, time_targets, T):\n        \"\"\"Initialization\"\"\" \n        self.T = T\n        x, y = np.array(time_dataset), np.array(time_targets)\n        self.length = len(x)\n        x = torch.from_numpy(x)\n        self.x = torch.unsqueeze(x, 1)\n        self.y = torch.from_numpy(y)\n\n    def __len__(self):\n        \"\"\"Denotes the total number of samples\"\"\"\n        return self.length\n\n    def __getitem__(self, index):\n        \"\"\"Generates samples of data\"\"\"\n        return self.x[index], self.y[index]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:49:15.314015Z","iopub.execute_input":"2023-11-28T12:49:15.314422Z","iopub.status.idle":"2023-11-28T12:49:15.322428Z","shell.execute_reply.started":"2023-11-28T12:49:15.314389Z","shell.execute_reply":"2023-11-28T12:49:15.321365Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"len(time_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T21:39:39.440180Z","iopub.execute_input":"2023-11-27T21:39:39.440857Z","iopub.status.idle":"2023-11-27T21:39:39.446541Z","shell.execute_reply.started":"2023-11-27T21:39:39.440823Z","shell.execute_reply":"2023-11-27T21:39:39.445585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:50:28.147901Z","iopub.execute_input":"2023-11-28T12:50:28.148872Z","iopub.status.idle":"2023-11-28T12:50:28.256677Z","shell.execute_reply.started":"2023-11-28T12:50:28.148829Z","shell.execute_reply":"2023-11-28T12:50:28.255616Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"int(np.floor(len(time_dataset) * 0.8))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T20:35:07.226956Z","iopub.execute_input":"2023-11-27T20:35:07.227661Z","iopub.status.idle":"2023-11-27T20:35:07.234216Z","shell.execute_reply.started":"2023-11-27T20:35:07.227628Z","shell.execute_reply":"2023-11-27T20:35:07.233273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation\n\nWe used no auction dataset that is normalised by decimal precision approach in their work. The first seven days are training data and the last three days are testing data. A validation set (20%) from the training set is used to monitor the overfitting behaviours.  \n\nThe first 40 columns of the FI-2010 dataset are 10 levels ask and bid information for a limit order book and we only use these 40 features in our network. The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons. ","metadata":{}},{"cell_type":"code","source":"batch_size = 8\n\ndataset_train = Dataset(time_dataset_train, time_targets_train, 100)\nprint('here')\ndataset_val = Dataset(time_dataset_val, time_targets_val, 100)\nprint('here1')\ndataset_test = Dataset(time_dataset_test, time_targets_test, 100)\nprint('here2')\ntrain_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\nprint('here3')\nval_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n\nprint(dataset_train.x.shape, dataset_train.y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:50:10.765932Z","iopub.execute_input":"2023-11-28T12:50:10.766348Z","iopub.status.idle":"2023-11-28T12:50:13.506889Z","shell.execute_reply.started":"2023-11-28T12:50:10.766319Z","shell.execute_reply":"2023-11-28T12:50:13.505866Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"here\nhere1\nhere2\nhere3\ntorch.Size([208899, 1, 100, 40]) torch.Size([208899])\n","output_type":"stream"}]},{"cell_type":"code","source":"tmp_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=1, shuffle=True)\nfor x, y in tmp_loader:\n    print(x)\n    print(y)\n    print(x.shape, y.shape)\n    break","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T12:50:16.542922Z","iopub.execute_input":"2023-11-28T12:50:16.543283Z","iopub.status.idle":"2023-11-28T12:50:16.668525Z","shell.execute_reply.started":"2023-11-28T12:50:16.543257Z","shell.execute_reply":"2023-11-28T12:50:16.667553Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tensor([[[[ 1.5993, -0.0842,  1.6167,  ..., -0.1342,  1.6128, -0.1460],\n          [ 1.5993, -0.0842,  1.6167,  ..., -0.1342,  1.6128, -0.1460],\n          [ 1.5993, -0.0842,  1.6167,  ..., -0.1342,  1.6128, -0.1460],\n          ...,\n          [ 1.5993, -0.0842,  1.6167,  ..., -0.1342,  1.5953, -0.0590],\n          [ 1.5993, -0.0842,  1.6167,  ..., -0.1342,  1.5953, -0.0590],\n          [ 1.5993, -0.0849,  1.6167,  ..., -0.1342,  1.5953, -0.0590]]]],\n       dtype=torch.float64)\ntensor([1])\ntorch.Size([1, 1, 100, 40]) torch.Size([1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Architecture\n\nPlease find the detailed discussion of our model architecture in our paper.","metadata":{}},{"cell_type":"code","source":"class deeplob(nn.Module):\n    def __init__(self, y_len):\n        super().__init__()\n        self.y_len = y_len\n        \n        # convolution blocks\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n            nn.LeakyReLU(negative_slope=0.01),\n#             nn.Tanh(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(32),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n            nn.Tanh(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n            nn.Tanh(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n            nn.Tanh(),\n            nn.BatchNorm2d(32),\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,10)),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(32),\n        )\n        \n        # inception moduels\n        self.inp1 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,1), padding='same'),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(64),\n        )\n        self.inp2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5,1), padding='same'),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(64),\n        )\n        self.inp3 = nn.Sequential(\n            nn.MaxPool2d((3, 1), stride=(1, 1), padding=(1, 0)),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.BatchNorm2d(64),\n        )\n        \n        # lstm layers\n        self.lstm = nn.LSTM(input_size=192, hidden_size=64, num_layers=1, batch_first=True)\n        self.fc1 = nn.Linear(64, self.y_len)\n\n    def forward(self, x):\n        # h0: (number of hidden layers, batch size, hidden size)\n        h0 = torch.zeros(1, x.size(0), 64).to(device)\n        c0 = torch.zeros(1, x.size(0), 64).to(device)\n    \n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        \n        x_inp1 = self.inp1(x)\n        x_inp2 = self.inp2(x)\n        x_inp3 = self.inp3(x)  \n        \n        x = torch.cat((x_inp1, x_inp2, x_inp3), dim=1)\n        \n#         x = torch.transpose(x, 1, 2)\n        x = x.permute(0, 2, 1, 3)\n        x = torch.reshape(x, (-1, x.shape[1], x.shape[2]))\n        \n        x, _ = self.lstm(x, (h0, c0))\n        x = x[:, -1, :]\n        x = self.fc1(x)\n        forecast_y = torch.softmax(x, dim=1)\n        \n        return forecast_y","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:50:30.346923Z","iopub.execute_input":"2023-11-28T12:50:30.347285Z","iopub.status.idle":"2023-11-28T12:50:30.369092Z","shell.execute_reply.started":"2023-11-28T12:50:30.347258Z","shell.execute_reply":"2023-11-28T12:50:30.368047Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = deeplob(3)\nmodel.to(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T12:50:31.008500Z","iopub.execute_input":"2023-11-28T12:50:31.008886Z","iopub.status.idle":"2023-11-28T12:50:37.371037Z","shell.execute_reply.started":"2023-11-28T12:50:31.008844Z","shell.execute_reply":"2023-11-28T12:50:37.370077Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"deeplob(\n  (conv1): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(1, 2), stride=(1, 2))\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n    (4): LeakyReLU(negative_slope=0.01)\n    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n    (7): LeakyReLU(negative_slope=0.01)\n    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 2))\n    (1): Tanh()\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n    (4): Tanh()\n    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n    (7): Tanh()\n    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv3): Sequential(\n    (0): Conv2d(32, 32, kernel_size=(1, 10), stride=(1, 1))\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n    (4): LeakyReLU(negative_slope=0.01)\n    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n    (7): LeakyReLU(negative_slope=0.01)\n    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (inp1): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=same)\n    (4): LeakyReLU(negative_slope=0.01)\n    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (inp2): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Conv2d(64, 64, kernel_size=(5, 1), stride=(1, 1), padding=same)\n    (4): LeakyReLU(negative_slope=0.01)\n    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (inp3): Sequential(\n    (0): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n    (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (lstm): LSTM(192, 64, batch_first=True)\n  (fc1): Linear(in_features=64, out_features=3, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"summary(model, (1, 1, 100, 40))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T12:50:37.373258Z","iopub.execute_input":"2023-11-28T12:50:37.373951Z","iopub.status.idle":"2023-11-28T12:50:40.851971Z","shell.execute_reply.started":"2023-11-28T12:50:37.373913Z","shell.execute_reply":"2023-11-28T12:50:40.850868Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\ndeeplob                                  [1, 3]                    --\n├─Sequential: 1-1                        [1, 32, 94, 20]           --\n│    └─Conv2d: 2-1                       [1, 32, 100, 20]          96\n│    └─LeakyReLU: 2-2                    [1, 32, 100, 20]          --\n│    └─BatchNorm2d: 2-3                  [1, 32, 100, 20]          64\n│    └─Conv2d: 2-4                       [1, 32, 97, 20]           4,128\n│    └─LeakyReLU: 2-5                    [1, 32, 97, 20]           --\n│    └─BatchNorm2d: 2-6                  [1, 32, 97, 20]           64\n│    └─Conv2d: 2-7                       [1, 32, 94, 20]           4,128\n│    └─LeakyReLU: 2-8                    [1, 32, 94, 20]           --\n│    └─BatchNorm2d: 2-9                  [1, 32, 94, 20]           64\n├─Sequential: 1-2                        [1, 32, 88, 10]           --\n│    └─Conv2d: 2-10                      [1, 32, 94, 10]           2,080\n│    └─Tanh: 2-11                        [1, 32, 94, 10]           --\n│    └─BatchNorm2d: 2-12                 [1, 32, 94, 10]           64\n│    └─Conv2d: 2-13                      [1, 32, 91, 10]           4,128\n│    └─Tanh: 2-14                        [1, 32, 91, 10]           --\n│    └─BatchNorm2d: 2-15                 [1, 32, 91, 10]           64\n│    └─Conv2d: 2-16                      [1, 32, 88, 10]           4,128\n│    └─Tanh: 2-17                        [1, 32, 88, 10]           --\n│    └─BatchNorm2d: 2-18                 [1, 32, 88, 10]           64\n├─Sequential: 1-3                        [1, 32, 82, 1]            --\n│    └─Conv2d: 2-19                      [1, 32, 88, 1]            10,272\n│    └─LeakyReLU: 2-20                   [1, 32, 88, 1]            --\n│    └─BatchNorm2d: 2-21                 [1, 32, 88, 1]            64\n│    └─Conv2d: 2-22                      [1, 32, 85, 1]            4,128\n│    └─LeakyReLU: 2-23                   [1, 32, 85, 1]            --\n│    └─BatchNorm2d: 2-24                 [1, 32, 85, 1]            64\n│    └─Conv2d: 2-25                      [1, 32, 82, 1]            4,128\n│    └─LeakyReLU: 2-26                   [1, 32, 82, 1]            --\n│    └─BatchNorm2d: 2-27                 [1, 32, 82, 1]            64\n├─Sequential: 1-4                        [1, 64, 82, 1]            --\n│    └─Conv2d: 2-28                      [1, 64, 82, 1]            2,112\n│    └─LeakyReLU: 2-29                   [1, 64, 82, 1]            --\n│    └─BatchNorm2d: 2-30                 [1, 64, 82, 1]            128\n│    └─Conv2d: 2-31                      [1, 64, 82, 1]            12,352\n│    └─LeakyReLU: 2-32                   [1, 64, 82, 1]            --\n│    └─BatchNorm2d: 2-33                 [1, 64, 82, 1]            128\n├─Sequential: 1-5                        [1, 64, 82, 1]            --\n│    └─Conv2d: 2-34                      [1, 64, 82, 1]            2,112\n│    └─LeakyReLU: 2-35                   [1, 64, 82, 1]            --\n│    └─BatchNorm2d: 2-36                 [1, 64, 82, 1]            128\n│    └─Conv2d: 2-37                      [1, 64, 82, 1]            20,544\n│    └─LeakyReLU: 2-38                   [1, 64, 82, 1]            --\n│    └─BatchNorm2d: 2-39                 [1, 64, 82, 1]            128\n├─Sequential: 1-6                        [1, 64, 82, 1]            --\n│    └─MaxPool2d: 2-40                   [1, 32, 82, 1]            --\n│    └─Conv2d: 2-41                      [1, 64, 82, 1]            2,112\n│    └─LeakyReLU: 2-42                   [1, 64, 82, 1]            --\n│    └─BatchNorm2d: 2-43                 [1, 64, 82, 1]            128\n├─LSTM: 1-7                              [1, 82, 64]               66,048\n├─Linear: 1-8                            [1, 3]                    195\n==========================================================================================\nTotal params: 143,907\nTrainable params: 143,907\nNon-trainable params: 0\nTotal mult-adds (M): 35.53\n==========================================================================================\nInput size (MB): 0.02\nForward/backward pass size (MB): 4.97\nParams size (MB): 0.58\nEstimated Total Size (MB): 5.56\n=========================================================================================="},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:50:40.853482Z","iopub.execute_input":"2023-11-28T12:50:40.853922Z","iopub.status.idle":"2023-11-28T12:50:40.860125Z","shell.execute_reply.started":"2023-11-28T12:50:40.853882Z","shell.execute_reply":"2023-11-28T12:50:40.859071Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Model Training\n","metadata":{}},{"cell_type":"code","source":"# A function to encapsulate the training loop\ndef batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):\n    \n    train_losses = np.zeros(epochs)\n    test_losses = np.zeros(epochs)\n    best_test_loss = np.inf\n    best_test_epoch = 0\n\n    for it in tqdm(range(epochs)):\n        \n        model.train()\n        t0 = datetime.now()\n        train_loss = []\n        for inputs, targets in train_loader:\n            # move data to GPU\n            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n            # print(\"inputs.shape:\", inputs.shape)\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            # Forward pass\n            # print(\"about to get model output\")\n            outputs = model(inputs)\n            # print(\"done getting model output\")\n            # print(\"outputs.shape:\", outputs.shape, \"targets.shape:\", targets.shape)\n            loss = criterion(outputs, targets)\n            # Backward and optimize\n            # print(\"about to optimize\")\n            loss.backward()\n            optimizer.step()\n            train_loss.append(loss.item())\n        # Get train loss and test loss\n        train_loss = np.mean(train_loss) # a little misleading\n    \n        model.eval()\n        test_loss = []\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)      \n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            test_loss.append(loss.item())\n        test_loss = np.mean(test_loss)\n\n        # Save losses\n        train_losses[it] = train_loss\n        test_losses[it] = test_loss\n        \n        if test_loss < best_test_loss:\n            torch.save(model, './best_val_model_pytorch')\n            best_test_loss = test_loss\n            best_test_epoch = it\n            print('model saved')\n\n        dt = datetime.now() - t0\n        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n          Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n\n    return train_losses, test_losses","metadata":{"execution":{"iopub.status.busy":"2023-11-28T12:50:42.799134Z","iopub.execute_input":"2023-11-28T12:50:42.799517Z","iopub.status.idle":"2023-11-28T12:50:42.811457Z","shell.execute_reply.started":"2023-11-28T12:50:42.799488Z","shell.execute_reply":"2023-11-28T12:50:42.810328Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train_losses, val_losses = batch_gd(model, criterion, optimizer, \n                                    train_loader, val_loader, epochs=50)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T12:50:43.829660Z","iopub.execute_input":"2023-11-28T12:50:43.830697Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  2%|▏         | 1/50 [04:05<3:20:29, 245.50s/it]","output_type":"stream"},{"name":"stdout","text":"model saved\nEpoch 1/50, Train Loss: 0.9333,           Validation Loss: 0.9956, Duration: 0:04:05.498214, Best Val Epoch: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'deeplob_our_data')","metadata":{"execution":{"iopub.status.busy":"2023-11-28T01:11:57.572296Z","iopub.execute_input":"2023-11-28T01:11:57.572633Z","iopub.status.idle":"2023-11-28T01:11:57.588962Z","shell.execute_reply.started":"2023-11-28T01:11:57.572606Z","shell.execute_reply":"2023-11-28T01:11:57.588045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.plot(train_losses, label='train loss')\nplt.plot(val_losses, label='validation loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T01:11:57.590167Z","iopub.execute_input":"2023-11-28T01:11:57.590989Z","iopub.status.idle":"2023-11-28T01:11:57.952988Z","shell.execute_reply.started":"2023-11-28T01:11:57.590952Z","shell.execute_reply":"2023-11-28T01:11:57.952090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Testing","metadata":{}},{"cell_type":"code","source":"model = torch.load('best_val_model_pytorch')\n\nn_correct = 0.\nn_total = 0.\nfor inputs, targets in test_loader:\n    # Move to GPU\n    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n\n    # Forward pass\n    outputs = model(inputs)\n    \n    # Get prediction\n    # torch.max returns both max and argmax\n    _, predictions = torch.max(outputs, 1)\n\n    # update counts\n    n_correct += (predictions == targets).sum().item()\n    n_total += targets.shape[0]\n\ntest_acc = n_correct / n_total\nprint(f\"Test acc: {test_acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T01:11:57.955557Z","iopub.execute_input":"2023-11-28T01:11:57.955937Z","iopub.status.idle":"2023-11-28T01:12:39.399757Z","shell.execute_reply.started":"2023-11-28T01:11:57.955904Z","shell.execute_reply":"2023-11-28T01:12:39.398857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = torch.load('best_val_model_pytorch')\nall_targets = []\nall_predictions = []\n\nfor inputs, targets in test_loader:\n    # Move to GPU\n    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n\n    # Forward pass\n    outputs = model(inputs)\n    \n    # Get prediction\n    # torch.max returns both max and argmax\n    _, predictions = torch.max(outputs, 1)\n\n    all_targets.append(targets.cpu().numpy())\n    all_predictions.append(predictions.cpu().numpy())\n\nall_targets = np.concatenate(all_targets)    \nall_predictions = np.concatenate(all_predictions)    ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T01:12:39.400969Z","iopub.execute_input":"2023-11-28T01:12:39.401271Z","iopub.status.idle":"2023-11-28T01:13:21.863804Z","shell.execute_reply.started":"2023-11-28T01:12:39.401243Z","shell.execute_reply":"2023-11-28T01:13:21.862957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('accuracy_score:', accuracy_score(all_targets, all_predictions))\nprint(classification_report(all_targets, all_predictions, digits=4))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T01:13:21.864987Z","iopub.execute_input":"2023-11-28T01:13:21.865278Z","iopub.status.idle":"2023-11-28T01:13:21.983462Z","shell.execute_reply.started":"2023-11-28T01:13:21.865253Z","shell.execute_reply":"2023-11-28T01:13:21.982460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}